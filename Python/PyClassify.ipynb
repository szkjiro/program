{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ここでは[by @al_log](https://qiita.com/ao_log/items/fe9bd42fd249c2a7ee7a)氏によるサンプルプログラムをもとに，\n",
        "機械学習手法のいろいろを，IRISデータをもとに試みます．\n",
        "\n",
        "Pythonのsci-kit learnライブラリを利用すれば，どの方法も10行とかからずに実行することができます．\n",
        "分類できた，という結果のみに注目するのであれば，ここにあげたような多くの手法を必要とはしません．\n",
        "重要なのは，データの特性に応じて適切な分類方法が決まってくること，\n",
        "ある方法で分類できたからといってそれで終わりではないことを知っておくことです．\n",
        "方法の違いの理解には，数学的準備およびプログラミングスキルがある程度必要になるところを説明抜きに，グラフ表現を通じて分類結果の違いとして観察します．\n",
        "\n",
        "グラフ表示を豊かにするために，はじめに表示関数を準備し，その実行環境にseabornライブラリを利用しています．\n",
        "\n",
        "最初にIRISデータセットを準備します．部分的には，クラスター分析のときに扱ったものと変わりません．"
      ],
      "metadata": {
        "id": "UWYfTGXsZhqz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtcFGD3MWb3N"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "# 標本の大きさ（サイズ）の確認\n",
        "print(iris.data.shape)\n",
        "# 各列ラベル（アヤメの種類）の表示\n",
        "print(iris.target_names)\n",
        "# zip関数により表示を縮約しています\n",
        "for data, target in zip(iris.data[:5], iris.target[:5]):\n",
        "  print(data, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "読み込んだデータセットをデータフレームの形式で使えるようにします．"
      ],
      "metadata": {
        "id": "NMLKPEufavFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "# データフレーム各列のラベルを設定\n",
        "df['target'] = iris.target\n",
        "df.loc[df['target'] == 0, 'target'] = \"setosa\"\n",
        "df.loc[df['target'] == 1, 'target'] = \"versicolor\"\n",
        "df.loc[df['target'] == 2, 'target'] = \"virginica\"\n",
        "\n",
        "# データフレームの始めの数行を表示\n",
        "df.head()"
      ],
      "metadata": {
        "id": "EvvkRIv3XkmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IRISデータセットの特徴を品種ごとの色分けをし，散布図行列に図示します．対角線には各データごとの分布を図示します．"
      ],
      "metadata": {
        "id": "QCMjVkKwa4rM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 散布図行列の表示\n",
        "import seaborn as sns\n",
        "\n",
        "sns.pairplot(df, hue=\"target\")"
      ],
      "metadata": {
        "id": "4OS4EiapX1AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YkBGi0LWaruh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "機械学習分類手法の特徴を色分けで示しやすくするための，表示関数を準備します．\n",
        "プログラミング的にはここが最も複雑になっています．"
      ],
      "metadata": {
        "id": "-7qPf9jBbAh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Xとyの設定\n",
        "X = iris.data[:, [0, 2]]\n",
        "y = iris.target\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# グラフの共通設定\n",
        "h = .02  # Xとyの値に応じて描画範囲を定める\n",
        "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "def decision_boundary(clf, X, y, ax, title):\n",
        "    clf.fit(X, y)\n",
        "\n",
        "    # 値 [x_min, x_max]x[y_min, y_max] の範囲で色の設定\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    # 塗りつぶし色の決定\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    ax.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "    # 散布図の点を描画\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n",
        "\n",
        "    # ラベル表示\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('sepal length')\n",
        "    ax.set_ylabel('petal length')"
      ],
      "metadata": {
        "id": "zKho5SSuYHkh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "k近傍法で分類します．\n",
        "k近傍法はよく使われる手法であり，k組に分類します．\n",
        "何組に分ける（kの値設定）かは自明なことではありません．\n",
        "以下ではkを4通りに試し，それぞれグラフに描画しています．"
      ],
      "metadata": {
        "id": "MIoY2_Tkbvro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# k-近傍法\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# 近傍数の設定\n",
        "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
        "\n",
        "for ax, n_neighbors in zip(axes, [1, 3, 6, 10]):\n",
        "    title = \"%s neighbor(s)\"% (n_neighbors)\n",
        "    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "    decision_boundary(clf, X, y, ax, title)"
      ],
      "metadata": {
        "id": "ntc-6m_zYUIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ある種の直線で区切られるように分類します．\n",
        "グラフではその境界が直線になって描画してあります．\n",
        "定数Cはロジスティク回帰の直線らしきものを定義するパラメータです．"
      ],
      "metadata": {
        "id": "z4V4WyIHc1yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ロジスティック回帰，分類境界には直線性が現れる\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
        "\n",
        "for ax, C in zip(axes, [0.01, 1, 100]):\n",
        "    title = \"C=%s\"% (C)\n",
        "    clf = LogisticRegression(C=C)\n",
        "    decision_boundary(clf, X, y, ax, title)"
      ],
      "metadata": {
        "id": "bKvIpihvYkYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "サポートベクトルマシンとは高次元空間の中でベクトルを分類する手法です．\n",
        "1980年から90年にかけての第2次人工知能ブームにおいて，ニューラルネットワークでは当時の計算機能力において計算しきれない問題をうまく処理するために開発が進みました．\n",
        "分類境界は直線（超平面）になります．\n",
        "\n",
        "深層学習（ディープラーニングがある程度実用になった今日においても，データの特徴が合えば，分類の解釈もわかりやすく有用な方法として使われ続けています．\n"
      ],
      "metadata": {
        "id": "Bm8WfCG7d2EJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# サポートベクトルマシーン，点間の余白を大きくする\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
        "\n",
        "for ax, C in zip(axes, [0.01, 1, 100]):\n",
        "    title = \"C=%s\"% (C)\n",
        "    clf = LinearSVC(C=C)\n",
        "    decision_boundary(clf, X, y, ax, title)"
      ],
      "metadata": {
        "id": "z72oR_RCYsp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "カーネル法とは，上の2つの方法が直線境界で分類していたものを曲線境界で分類可能にする方法です．\n",
        "直線に関するパラメータCに加えて曲線に関するパラメータγ（ガンマ）があります．"
      ],
      "metadata": {
        "id": "QMWzp94ne7Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# カーネル法をあわせたサポートベクトルマシーン，境界がカーネル関数を反映した形になる\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
        "\n",
        "for ax_row, C in zip(axes, [0.01, 1, 100]):\n",
        "    for ax, gamma in zip(ax_row, [0.1, 1, 10]):\n",
        "        title = \"C=%s, gamma=%s\"% (C, gamma)\n",
        "        clf = SVC(C=C, gamma=gamma)\n",
        "        decision_boundary(clf, X, y, ax, title)"
      ],
      "metadata": {
        "id": "IEeH5wLZY1GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "決定木による分類です．\n",
        "クラスター分析で扱ったデンドログラムの作成にもとづく方法です．\n",
        "平面に図示できる場合，境界は縦横の直線で順次分けて行くように描画されます．"
      ],
      "metadata": {
        "id": "5jdQzNGYfbtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 決定木，縦横に直線で次々と切り分けるイメージ\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
        "\n",
        "for ax, max_depth in zip(axes, [1, 3, 8]):\n",
        "    title = \"max_depth=%s\"% (max_depth)\n",
        "    clf = DecisionTreeRegressor(max_depth=max_depth)\n",
        "    decision_boundary(clf, X, y, ax, title)"
      ],
      "metadata": {
        "id": "LpJFn2FKY_PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ランダムフォレスト（偶然性によるいくつもの分類を集めたもの）は，二分木で「木（Tree）」と呼んだことに応じて，「森（Forest）＝木を集めたもの」を作り，森から木を分類していくことを指した名前です．\n",
        "\n"
      ],
      "metadata": {
        "id": "p0HbyfcYfxoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ランダムフォレストを用いた決定木，いくつかの分類候補（ランダムに作る）から良いものを選ぶ\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(7, 3))\n",
        "clfs = [RandomForestClassifier(), GradientBoostingClassifier()]\n",
        "titles = [\"RandomForestClassifier\", \"GradientBoostingClassifier\"]\n",
        "\n",
        "for ax, clf, title in zip(axes, clfs, titles):\n",
        "    decision_boundary(clf, X, y, ax, title)"
      ],
      "metadata": {
        "id": "xIbYxatvZF3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットワークによる分類です．\n",
        "この例題では単に計算時間が多くかかるデメリットばかりです．\n",
        "問題の性質が上で扱ってきた，ある種の線形性にもとづく分類ができる（上の散布図行列で直感的に分類できそう）問題では効果はありませんが，分類するための情報に欠ける問題では有効性が示されました．\n",
        "\n",
        "そのことが今日の深層学習の隆盛につながっています．"
      ],
      "metadata": {
        "id": "zLHtMK21gZas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ニューラルネットワーク，データ数が少ないので線形分類とあまり変わらない\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
        "\n",
        "for ax, n in zip(axes, [15, 15, 15, 15]):\n",
        "    title = \"\"\n",
        "    clf = MLPClassifier(hidden_layer_sizes=[n, n])\n",
        "    decision_boundary(clf, X, y, ax, title)"
      ],
      "metadata": {
        "id": "OckyBR9TZOfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "k近傍法では距離の問題に応じて距離などをうまく設定する，分類を進める方法をより適切にとる，などにより有効性が向上します．\n",
        "いくつ（k）に分類したいという目的がある場合，有効性の高い方法です．\n",
        "以下のプログラムは最初のk近傍法のバリエーションの一つです．\n",
        "\n",
        "以下ではkが2から5まで出力しています．"
      ],
      "metadata": {
        "id": "WLYAg5UihLWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# k-平均法，点群の重心が離れるようにする\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
        "\n",
        "for ax, n_clusters in zip(axes, [2, 3, 4, 5]):\n",
        "    title = \"n_clusters=%s\"% (n_clusters)\n",
        "    clf = KMeans(n_clusters=n_clusters)\n",
        "    decision_boundary(clf, X, y, ax, title)"
      ],
      "metadata": {
        "id": "FJFU5IgAZVZR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}