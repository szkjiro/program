{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/szkjiro/program/blob/main/PyWord2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ワードクラウドの作成において，以前に形態素解析を扱いました．\n",
        "次のプログラムはMeCabを用いて例文「失敗に挑戦しないことが最大の失敗である」\n",
        "の形態素解析の実行部分だけを抜き出しています．\n",
        "MeCabを使うために，最初の行はMeCab辞書をインストールしています．"
      ],
      "metadata": {
        "id": "Sg2iIPFCV-wb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kD8vo4c6zs8Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmrrdS4TV18A"
      },
      "outputs": [],
      "source": [
        "!pip install mecab-python3 unidic-lite\n",
        "import MeCab\n",
        "# parse は，文章の文法解析をする一般的なことば\n",
        "print(MeCab.Tagger().parse(\"失敗に挑戦しないことが最大の失敗である\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "自然言語処理のためによく使われる表現に**One-hotベクトル**があります．\n",
        "上の解析例でいえば，次のような0と1からなる数の並び（ベクトル）を対応させます．\n",
        "1が一回だけ現れ，残りは0であることからつけられた名前です．\n",
        "見てわかるように「何番目に何という単語がある」という情報をベクトルの形に書き換えているだけです．\n",
        "形態素解析の切り出した単語は全部で12個です．\n",
        "\n",
        "- 失敗 (1,0,0,0,0,0,0,0,0,0,0,0)\n",
        "- に (0,1,0,0,0,0,0,0,0,0,0,0)\n",
        "- 挑戦 (0,0,1,0,0,0,0,0,0,0,0,0)\n",
        "- し (0,0,0,1,0,0,0,0,0,0,0,0)\n",
        "- ない (0,0,0,0,1,0,0,0,0,0,0,0)\n",
        "- こと (0,0,0,0,0,1,0,0,0,0,0,0)\n",
        "- が (0,0,0,0,0,0,1,0,0,0,0,0)\n",
        "- 最大 (0,0,0,0,0,0,0,1,0,0,0,0)\n",
        "- の (0,0,0,0,0,0,0,0,1,0,0,0)\n",
        "- 失敗 (0,0,0,0,0,0,0,0,0,1,0,0)\n",
        "- で (0,0,0,0,0,0,0,0,0,0,1,0)\n",
        "- ある (0,0,0,0,0,0,0,0,0,0,0,1)\n",
        "\n",
        "同じ単語「失敗」でも，現れる位置により異なるベクトルに作られています．\n",
        "このような単語と位置の組み合わせから，**n-gram**を作成することもできます．\n",
        "n-gramは隣接するn個の単語が何回登場するかの統計情報を表すものです．\n",
        "\n",
        "## 共起関係とコーパス\n",
        "\n",
        "このOne-hotベクトル表現は，扱う文章が長くなればなるほど，ベクトル（の数の並び）も長くなります．\n",
        "n-gramだけでは単語もしくは複数の単語の並びの統計情報だけなので，\n",
        "たとえば「失敗という単語から10語以内に成功という単語が現れやすい\n",
        "（仮の主張なので本当ではないと思います）」のようなことはわかりません．\n",
        "ここに書いた何語以内に特定の単語同士が現れやすい関係を**共起関係**といい，\n",
        "言語学者はそうした情報を整備した**コーパス（辞書）**を従来から整備してきました．\n",
        "\n",
        "## 分散表現\n",
        "\n",
        "One-hotベクトルのような言語ベクトルから，単語だけをうまく扱う方法はないのでしょうか．\n",
        "そこで考えられたものの一つが分散表現です．\n",
        "分散表現では意味の似た単語はベクトルとして近くに位置し，\n",
        "似てないものは遠くに位置するものと考えます．\n",
        "またベクトル長は固定長であり，計算処理も扱いやすくします．\n",
        "\n",
        "このような都合のよいベクトルを実現する方法があるのでしょうか．\n",
        "その方法の一つが，ニューラルネットワークによる機械学習を通じて，\n",
        "One-hotベクトルから分散表現を作り出す**Word2Vec**でした．\n",
        "Word2vecは，Googleの研究者であるトマス・ミコロフの研究チームにより，\n",
        "2013年に公開されました．\n",
        "そこではスキップグラム法（skip-gram法）およびCBOW(continuous bag-of-words)という2つの技術を組み合わせています．\n",
        "いずれも教師あり機械学習が用いられています．\n",
        "\n",
        "これを実行する準備に，日本語Wikipediaから作成済みの言語モデルなどをインストールします．2分以上かかるでしょう．"
      ],
      "metadata": {
        "id": "EovcgWGzWrY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[ja]\n",
        "!wget https://github.com/singletongue/WikiEntVec/releases/download/20190520/jawiki.word_vectors.200d.txt.bz2\n",
        "!bzip2 -d jawiki.word_vectors.200d.txt.bz2"
      ],
      "metadata": {
        "id": "2X5VJ44Gdl08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "言語モデルはgensimというライブラリを利用します．\n",
        "2行目の右辺になんとか.txtというファイル名があります．\n",
        "これがWikipediaから作成されたテキスト情報です．\n",
        "きわめて巨大なデータファイルなので，表示させるとたいへんなことになります．2分以上かかるでしょう．"
      ],
      "metadata": {
        "id": "2zDU148Ae8yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import models\n",
        "w2v_model =  models.KeyedVectors.load_word2vec_format('jawiki.word_vectors.200d.txt', binary=False)"
      ],
      "metadata": {
        "id": "hmyU5IZ0fKl3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "信州という単語の内容を見てみましょう．"
      ],
      "metadata": {
        "id": "I24UNzv-qHOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"信州\"\n",
        "\n",
        "# Word2Vecで作成した単語ベクトルの値\n",
        "word_vec = w2v_model.__getitem__(word)\n",
        "print(word_vec.shape)\n",
        "\n",
        "# Word2Vecで作成した単語ベクトル\n",
        "print(word_vec)"
      ],
      "metadata": {
        "id": "cYPAEMkTqMVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次では信州に似ている単語を順に表示させます．"
      ],
      "metadata": {
        "id": "1vaFSHHsrwB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Similars = w2v_model.most_similar(positive=['信州'])\n",
        "for Niteiru in Similars:\n",
        "    print(Niteiru)"
      ],
      "metadata": {
        "id": "SqmE4Oh8r2eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "各単語の右に現れた数字は基準とした単語「信州」との相関係数と呼んでいいものです．\n",
        "すでに観察した信州に対する小数値の並びのベクトル同士の内積の値でもあります．\n",
        "\n",
        "2つのベクトル$a$と$b$のなす角が$\\theta$であるとき，その内積に関して\n",
        "$$\n",
        "a\\cdot b = |a|\\cdot |b|\\cos\\theta\n",
        "$$\n",
        "という関係式があることを覚えている人はいるでしょう．\n",
        "この式を変形すれば\n",
        "$$\n",
        "\\cos\\theta = \\frac{a\\cdot b}{|a|\\cdot |b|}\n",
        "$$\n",
        "となり，この式が2つのベクトルの相関係数を与えています．\n",
        "この式にもとづき，上で見た相関係数を**コサイン類似度**とも呼びます．\n",
        "ベクトルのことばでいうと，2つのベクトルが似ているほど，2つのベクトルのなす角度が0に近いのです．\n",
        "\n",
        "この類似単語の表示は，複数の単語に対しても可能です．"
      ],
      "metadata": {
        "id": "HgCTPC8tsQVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Similars = w2v_model.most_similar(positive=['長野県', 'サッカー'])\n",
        "for Niteiru in Similars:\n",
        "    print(Niteiru)"
      ],
      "metadata": {
        "id": "Ox6gQXBMuU3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "単語同士の演算もできます．\n",
        "ベクトルで表されているからです．\n",
        "positiveに指定した単語から，negativeに指定した単語（複数指定可能，上の例を参考）をベクトルとして計算した結果を類似度順に表示します．"
      ],
      "metadata": {
        "id": "TV-v3yLQuuKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Keisan = w2v_model.most_similar(positive=['信州','グルメ'], negative=['ラーメン'])\n",
        "for Atai in Keisan:\n",
        "    print(Atai)"
      ],
      "metadata": {
        "id": "i1r6mi1kuy_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatGPTとは何なのか\n",
        "\n",
        "現在，世界的にChatGPTの利用に注目が集まっています．\n",
        "ChatGPTは大規模言語モデル（LLM; Large-scale Language Model）の一つGPT (Generative Pre-trained Transformer) をもとに開発されました．\n",
        "TransformerはWord2Vecの欠点を改良する注意機構という仕組みであり，2017年にGoogleが公開しました．\n",
        "これはGoogleが開発したLLMであるBERTに組み込まれています．\n",
        "このためLLMに関する公開情報ではBERTに関する解説（書籍もあり）が，最も包括的に書かれています．\n",
        "上のインストールの仕方にtransformerの名前があったのは，現在では，Word2Vecがその一部として提供されているためです．\n",
        "\n",
        "大きな改良がなされたとはいえ，GPTはWord2Vecをもとにした分散表現の一種です．\n",
        "このためChatGPTが綴る文章は，上の例題で扱ってきた確率にしたがって文章を並べる仕組みなのです．"
      ],
      "metadata": {
        "id": "Fhaw0IyPxYED"
      }
    }
  ]
}