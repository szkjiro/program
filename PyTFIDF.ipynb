{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/szkjiro/program/blob/main/PyTFIDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 古くからある文章解析\n",
        "\n",
        "TF（Term Frequency）は古くからある文章解析技術です．\n",
        "単語の意味から容易に想像できるように，ある文章中の単語の出現頻度を表します．\n",
        "ワードクラウドはTFに応じて文字を大きくする表示を組み合わせた表現の一種なのです．\n",
        "\n",
        "インターネット検索の世界でも，Google検索の登場した1998年以前は，このTFにもとづき複数文書を比較可能にしたTF-IDF，それはTFにIDFと呼ばれる量の積をとった対数値のことです．\n",
        "TFを用いている以上の説明は本授業のレベルを超えます．たとえば[初学者向けTFIDFについて簡単にまとめてみた](https://www.takapy.work/entry/2019/01/14/141423)などを参照してください．\n",
        "このTF-IDFにより，ウェブサイトの順位付けをしていた検索サイトがほとんでした．\n",
        "学術文書などでは，このTF-IDFによる順位付けは比較的うまく機能していました．\n",
        "それが販売促進用途などでは，このTF-IDFの値を高くするためだけの邪道ともいえるテクニックにより無効化しつつあったのです．\n",
        "\n",
        "歴史的にこのようなことがあったとはいえ，文章解析におけるTF-IDFの有効性が失われたわけではありません．\n",
        "ここで扱うのはTF-IDFをPythonの機械学習ライブラリscikit-learnを用いて計算してみます．\n",
        "機械学習ライブラリscikit-learnは今後も何回か登場します．\n",
        "\n",
        "## 形態素解析不要のTF-IDF\n",
        "\n",
        "まずは，分かち書きしてある文を直接に与えてTF-IDFの値を計算するところを観察します．\n",
        "この例文は上にあるリンク先と同じです．\n",
        "この例における分かち書き済の文の代わりに，青空文庫から取得した文章の分かち書きを与えれば，TF-IDFを計算できるだろうことは想像できるでしょう．\n",
        "\n",
        "途中の説明にベクトルのことばが何回も出てきます．\n",
        "自然言語処理では文中の単語ごとの頻度や位置に関する情報をベクトルの形で利用するためです．\n",
        "\n",
        "言語をベクトルで扱う話は，後に現在世界的に注目のChatGPTに関係した話題を扱うために再び戻ります．"
      ],
      "metadata": {
        "id": "VS2GQQWA7U1b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fED7I9qo7UKI"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "# A, Bの2つの文を準備する\n",
        "df = pd.DataFrame({'id': ['A', 'B'],\n",
        "                   'text': [ '私は ラーメン 愛する 中でも 味噌 ラーメン 一番 好き', '私は 焼きそば 好き しかし ラーメン もっと 好き']})\n",
        "\n",
        "\n",
        "# TF-IDFの計算\n",
        "tfidf_vectorizer = TfidfVectorizer(use_idf=True,lowercase=False)\n",
        "\n",
        "# 文章内の全単語のTfidf値を取得\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
        "\n",
        "# index 順の単語リスト\n",
        "terms = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# 単語毎のtfidf値配列：TF-IDF 行列 (numpy の ndarray 形式で取得される)\n",
        "# 1つ目の文書に対する、各単語のベクトル値\n",
        "# 2つ目の文書に対する、各単語のベクトル値\n",
        "# ・・・\n",
        "# が取得できる（文書の数 * 全単語数）の配列になる。（toarray()で密行列に変換）\n",
        "tfidfs = tfidf_matrix.toarray()\n",
        "\n",
        "# 計算結果の確認\n",
        "# 分かち書きの単語一覧をまず表示\n",
        "print(terms)\n",
        "# 各単語に対応した計算値の表示\n",
        "print(tfidfs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上の計算結果のうち，Aの文では計算値の一番大きいのが「ラーメン」，Bの文では一番大きいのが「好き」です．\n",
        "これらが，それぞれの文の特徴であると見ます．\n",
        "文章をベクトルで扱うことができることだけ頭の片隅においといてください．\n",
        "\n",
        "## この後どうする？\n",
        "\n",
        "青空文庫にある２作品の分かち書きを，上の例と同様に与えれば，作品比較ができます．\n",
        "授業の今の段階で解説は厳しそうなので，ChatGPTの話題に触れる回まで先送りします．"
      ],
      "metadata": {
        "id": "EwPfIkrHDzoY"
      }
    }
  ]
}